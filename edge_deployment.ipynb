{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e2c156-7d04-44c9-b01f-031ebf900aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "\n",
    "class BandwidthPredictor:\n",
    "    def __init__(self, models_dir=\"tflite_models\", metadata_dir=\"metadata\"):\n",
    "        \"\"\"\n",
    "        Initialize the bandwidth predictor with models and metadata for edge deployment\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        models_dir : str\n",
    "            Directory containing TFLite models\n",
    "        metadata_dir : str\n",
    "            Directory containing metadata files\n",
    "        \"\"\"\n",
    "        self.models_dir = models_dir\n",
    "        self.metadata_dir = metadata_dir\n",
    "        \n",
    "        # Load metadata\n",
    "        self._load_metadata()\n",
    "        \n",
    "        # Load TFLite models\n",
    "        self._load_models()\n",
    "        \n",
    "        print(f\"Loaded {len(self.models)} service group models\")\n",
    "        print(f\"Available service groups: {list(self.models.keys())}\")\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        \"\"\"Load metadata files for prediction\"\"\"\n",
    "        try:\n",
    "            # Load service mappings\n",
    "            with open(os.path.join(self.metadata_dir, \"service_mappings.json\"), \"r\") as f:\n",
    "                self.service_mappings = json.load(f)\n",
    "            \n",
    "            # Load service group stats\n",
    "            with open(os.path.join(self.metadata_dir, \"service_group_stats.json\"), \"r\") as f:\n",
    "                self.service_group_stats = json.load(f)\n",
    "                \n",
    "            # Load feature list\n",
    "            with open(os.path.join(self.metadata_dir, \"feature_list.json\"), \"r\") as f:\n",
    "                self.feature_info = json.load(f)\n",
    "                \n",
    "            # Load scaler parameters\n",
    "            with open(os.path.join(self.metadata_dir, \"scalers.json\"), \"r\") as f:\n",
    "                scaler_params = json.load(f)\n",
    "                \n",
    "            # Initialize scalers for each service group\n",
    "            self.scalers = {}\n",
    "            for service_group, params in scaler_params.items():\n",
    "                feature_scaler = MinMaxScaler()\n",
    "                feature_scaler.min_ = np.array(params[\"features\"][\"min\"])\n",
    "                feature_scaler.scale_ = np.array(params[\"features\"][\"scale\"])\n",
    "                \n",
    "                target_scaler = MinMaxScaler()\n",
    "                target_scaler.min_ = np.array(params[\"target\"][\"min\"])\n",
    "                target_scaler.scale_ = np.array(params[\"target\"][\"scale\"])\n",
    "                \n",
    "                self.scalers[service_group] = {\n",
    "                    \"features\": feature_scaler,\n",
    "                    \"target\": target_scaler\n",
    "                }\n",
    "            \n",
    "            print(\"Successfully loaded metadata\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading metadata: {str(e)}\")\n",
    "    \n",
    "    def _load_models(self):\n",
    "        \"\"\"Load TFLite models\"\"\"\n",
    "        self.models = {}\n",
    "        self.interpreters = {}\n",
    "        self.input_details = {}\n",
    "        self.output_details = {}\n",
    "        \n",
    "        try:\n",
    "            # Check if models directory exists\n",
    "            if not os.path.exists(self.models_dir):\n",
    "                raise RuntimeError(f\"Models directory {self.models_dir} not found\")\n",
    "            \n",
    "            # Load each model file\n",
    "            for model_file in os.listdir(self.models_dir):\n",
    "                if model_file.endswith(\".tflite\"):\n",
    "                    # Extract service group from filename\n",
    "                    if 'fallback' in model_file:\n",
    "                        # Skip fallback models if regular ones exist\n",
    "                        service_group = model_file.split('_fallback_model')[0].replace('_', ' ').title()\n",
    "                        if service_group in self.models:\n",
    "                            continue\n",
    "                    else:\n",
    "                        service_group = model_file.split('_model')[0].replace('_', ' ').title()\n",
    "                    \n",
    "                    # Load model\n",
    "                    model_path = os.path.join(self.models_dir, model_file)\n",
    "                    \n",
    "                    # Read model file\n",
    "                    with open(model_path, 'rb') as f:\n",
    "                        model_content = f.read()\n",
    "                    \n",
    "                    # Create interpreter\n",
    "                    interpreter = tf.lite.Interpreter(model_content=model_content)\n",
    "                    interpreter.allocate_tensors()\n",
    "                    \n",
    "                    # Get input and output details\n",
    "                    input_details = interpreter.get_input_details()\n",
    "                    output_details = interpreter.get_output_details()\n",
    "                    \n",
    "                    # Store model\n",
    "                    self.models[service_group] = model_content\n",
    "                    self.interpreters[service_group] = interpreter\n",
    "                    self.input_details[service_group] = input_details\n",
    "                    self.output_details[service_group] = output_details\n",
    "                    \n",
    "                    print(f\"Loaded model for {service_group}\")\n",
    "            \n",
    "            if not self.models:\n",
    "                raise RuntimeError(\"No models found in the models directory\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading models: {str(e)}\")\n",
    "    \n",
    "    def generate_time_features(self, timestamp):\n",
    "        \"\"\"\n",
    "        Generate time-based features for a timestamp\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        timestamp : datetime\n",
    "            Timestamp to generate features for\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            Time features (hour, day_of_week, is_weekend, hour_features, day_features)\n",
    "        \"\"\"\n",
    "        hour = timestamp.hour\n",
    "        day_of_week = timestamp.weekday()\n",
    "        is_weekend = 1 if day_of_week >= 5 else 0\n",
    "        \n",
    "        # Hour one-hot encoding\n",
    "        hour_features = [1 if h == hour else 0 for h in range(24)]\n",
    "        \n",
    "        # Day one-hot encoding\n",
    "        day_features = [1 if d == day_of_week else 0 for d in range(7)]\n",
    "        \n",
    "        return hour, day_of_week, is_weekend, hour_features, day_features\n",
    "    \n",
    "    def prepare_features(self, timestamp, service_group, usage_percentage=None, device_group=None):\n",
    "        \"\"\"\n",
    "        Prepare input features for prediction\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        timestamp : datetime\n",
    "            Timestamp for prediction\n",
    "        service_group : str\n",
    "            Service group to predict for\n",
    "        usage_percentage : float, optional\n",
    "            Usage percentage (if known)\n",
    "        device_group : str, optional\n",
    "            Device group ('personal_device' or 'work_device')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy.ndarray\n",
    "            Scaled features for model input\n",
    "        \"\"\"\n",
    "        # Generate time features\n",
    "        hour, day_of_week, is_weekend, hour_features, day_features = self.generate_time_features(timestamp)\n",
    "        \n",
    "        # Generate or use provided usage percentage\n",
    "        if usage_percentage is None:\n",
    "            # Define usage patterns by service group and time of day\n",
    "            usage_patterns = {\n",
    "                \"Streaming\": {\n",
    "                    \"morning\": (5, 9, 30, 50),      # 5-9 AM: 30-50%\n",
    "                    \"workday\": (9, 17, 40, 60),     # 9 AM-5 PM: 40-60%\n",
    "                    \"evening\": (17, 23, 70, 90),    # 5-11 PM: 70-90%\n",
    "                    \"night\": (23, 5, 50, 70),       # 11 PM-5 AM: 50-70%\n",
    "                    \"weekend_boost\": 10             # +10% on weekends\n",
    "                },\n",
    "                \"Gaming\": {\n",
    "                    \"morning\": (5, 9, 20, 40),      # 5-9 AM: 20-40%\n",
    "                    \"workday\": (9, 17, 30, 50),     # 9 AM-5 PM: 30-50%\n",
    "                    \"evening\": (17, 23, 75, 95),    # 5-11 PM: 75-95%\n",
    "                    \"night\": (23, 5, 60, 80),       # 11 PM-5 AM: 60-80%\n",
    "                    \"weekend_boost\": 15             # +15% on weekends\n",
    "                },\n",
    "                \"Social Media\": {\n",
    "                    \"morning\": (5, 9, 50, 70),      # 5-9 AM: 50-70%\n",
    "                    \"workday\": (9, 17, 60, 80),     # 9 AM-5 PM: 60-80%\n",
    "                    \"evening\": (17, 23, 70, 90),    # 5-11 PM: 70-90%\n",
    "                    \"night\": (23, 5, 40, 60),       # 11 PM-5 AM: 40-60%\n",
    "                    \"weekend_boost\": 5              # +5% on weekends\n",
    "                },\n",
    "                \"Shopping\": {\n",
    "                    \"morning\": (5, 9, 30, 50),      # 5-9 AM: 30-50%\n",
    "                    \"workday\": (9, 17, 50, 70),     # 9 AM-5 PM: 50-70%\n",
    "                    \"evening\": (17, 23, 60, 80),    # 5-11 PM: 60-80%\n",
    "                    \"night\": (23, 5, 20, 40),       # 11 PM-5 AM: 20-40%\n",
    "                    \"weekend_boost\": 10             # +10% on weekends\n",
    "                },\n",
    "                \"Software\": {\n",
    "                    \"morning\": (5, 9, 60, 80),      # 5-9 AM: 60-80%\n",
    "                    \"workday\": (9, 17, 80, 95),     # 9 AM-5 PM: 80-95%\n",
    "                    \"evening\": (17, 23, 40, 60),    # 5-11 PM: 40-60%\n",
    "                    \"night\": (23, 5, 20, 40),       # 11 PM-5 AM: 20-40%\n",
    "                    \"weekend_boost\": -20            # -20% on weekends\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Use default pattern if service group not found\n",
    "            pattern = usage_patterns.get(service_group, {\n",
    "                \"morning\": (5, 9, 40, 60),\n",
    "                \"workday\": (9, 17, 50, 70),\n",
    "                \"evening\": (17, 23, 60, 80),\n",
    "                \"night\": (23, 5, 30, 50),\n",
    "                \"weekend_boost\": 5\n",
    "            })\n",
    "            \n",
    "            # Determine which time period we're in\n",
    "            if pattern[\"morning\"][0] <= hour < pattern[\"morning\"][1]:\n",
    "                min_usage, max_usage = pattern[\"morning\"][2], pattern[\"morning\"][3]\n",
    "            elif pattern[\"workday\"][0] <= hour < pattern[\"workday\"][1]:\n",
    "                min_usage, max_usage = pattern[\"workday\"][2], pattern[\"workday\"][3]\n",
    "            elif pattern[\"evening\"][0] <= hour < pattern[\"evening\"][1]:\n",
    "                min_usage, max_usage = pattern[\"evening\"][2], pattern[\"evening\"][3]\n",
    "            else:  # night\n",
    "                min_usage, max_usage = pattern[\"night\"][2], pattern[\"night\"][3]\n",
    "            \n",
    "            # Add weekend boost if applicable\n",
    "            if is_weekend:\n",
    "                min_usage += pattern[\"weekend_boost\"]\n",
    "                max_usage += pattern[\"weekend_boost\"]\n",
    "                \n",
    "            # Ensure within 0-100 range\n",
    "            min_usage = max(0, min(100, min_usage))\n",
    "            max_usage = max(0, min(100, max_usage))\n",
    "            \n",
    "            # Generate random usage percentage within the determined range\n",
    "            usage_percentage = random.uniform(min_usage, max_usage)\n",
    "        \n",
    "        # Determine device group if not provided\n",
    "        if device_group is None:\n",
    "            # Default logic: work devices more likely during work hours\n",
    "            if service_group == \"Software\" and 8 <= hour <= 18 and day_of_week < 5:\n",
    "                device_group_encoded = 1  # work device\n",
    "            else:\n",
    "                device_group_encoded = 0  # personal device\n",
    "        else:\n",
    "            # Use provided device group\n",
    "            device_group_encoded = 1 if device_group == \"work_device\" else 0\n",
    "        \n",
    "        # Generate network metrics based on service group and time\n",
    "        if service_group == \"Streaming\":\n",
    "            signal_strength = random.uniform(-70, -30)\n",
    "            packet_loss = random.uniform(0, 0.2)\n",
    "            latency = random.uniform(10, 50)\n",
    "            jitter = random.uniform(1, 5)\n",
    "        elif service_group == \"Gaming\":\n",
    "            signal_strength = random.uniform(-65, -25)\n",
    "            packet_loss = random.uniform(0, 0.1)\n",
    "            latency = random.uniform(5, 30)\n",
    "            jitter = random.uniform(0.5, 3)\n",
    "        elif service_group == \"Social Media\":\n",
    "            signal_strength = random.uniform(-75, -40)\n",
    "            packet_loss = random.uniform(0, 0.3)\n",
    "            latency = random.uniform(20, 80)\n",
    "            jitter = random.uniform(2, 8)\n",
    "        elif service_group == \"Shopping\":\n",
    "            signal_strength = random.uniform(-80, -45)\n",
    "            packet_loss = random.uniform(0, 0.4)\n",
    "            latency = random.uniform(30, 100)\n",
    "            jitter = random.uniform(3, 10)\n",
    "        else:  # Software\n",
    "            signal_strength = random.uniform(-70, -35)\n",
    "            packet_loss = random.uniform(0, 0.2)\n",
    "            latency = random.uniform(15, 70)\n",
    "            jitter = random.uniform(1, 7)\n",
    "            \n",
    "        # Calculate usage minutes based on usage percentage\n",
    "        usage_minutes = usage_percentage * 0.3\n",
    "        \n",
    "        # Build feature vector based on metadata\n",
    "        basic_features = self.feature_info[\"basic_features\"]\n",
    "        network_features = self.feature_info[\"network_features\"]\n",
    "        \n",
    "        # Basic features\n",
    "        feature_values = [usage_percentage]\n",
    "        if 'device_group_encoded' in basic_features:\n",
    "            feature_values.append(device_group_encoded)\n",
    "        \n",
    "        # Network features\n",
    "        network_values = []\n",
    "        if 'signal_strength' in network_features:\n",
    "            network_values.append(signal_strength)\n",
    "        if 'packet_loss' in network_features:\n",
    "            network_values.append(packet_loss)\n",
    "        if 'latency' in network_features:\n",
    "            network_values.append(latency)\n",
    "        if 'jitter' in network_features:\n",
    "            network_values.append(jitter)\n",
    "        if 'usage_minutes' in network_features:\n",
    "            network_values.append(usage_minutes)\n",
    "        \n",
    "        # Time features\n",
    "        time_values = [hour, day_of_week, is_weekend]\n",
    "        \n",
    "        # Combine all features\n",
    "        all_features = feature_values + network_values + time_values + hour_features + day_features\n",
    "        \n",
    "        # Scale features\n",
    "        feature_array = np.array(all_features).reshape(1, -1)\n",
    "        scaled_features = self.scalers[service_group][\"features\"].transform(feature_array)\n",
    "        \n",
    "        return scaled_features.astype(np.float32)\n",
    "    \n",
    "    def predict(self, timestamp, service_group, usage_percentage=None, device_group=None):\n",
    "        \"\"\"\n",
    "        Make prediction for a specific service group\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        timestamp : datetime\n",
    "            Timestamp for prediction\n",
    "        service_group : str\n",
    "            Service group to predict for\n",
    "        usage_percentage : float, optional\n",
    "            Usage percentage (if known)\n",
    "        device_group : str, optional\n",
    "            Device group ('personal_device' or 'work_device')\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Prediction results\n",
    "        \"\"\"\n",
    "        if service_group not in self.models:\n",
    "            raise ValueError(f\"No model available for service group: {service_group}\")\n",
    "        \n",
    "        # Prepare features\n",
    "        scaled_features = self.prepare_features(timestamp, service_group, usage_percentage, device_group)\n",
    "        \n",
    "        # Get interpreter\n",
    "        interpreter = self.interpreters[service_group]\n",
    "        input_details = self.input_details[service_group]\n",
    "        output_details = self.output_details[service_group]\n",
    "        \n",
    "        # Set input tensor\n",
    "        interpreter.set_tensor(input_details[0]['index'], scaled_features)\n",
    "        \n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "        \n",
    "        # Get output\n",
    "        scaled_prediction = interpreter.get_tensor(output_details[0]['index'])\n",
    "        \n",
    "        # Inverse transform to get original bandwidth scale\n",
    "        bandwidth_prediction = self.scalers[service_group]['target'].inverse_transform(\n",
    "            scaled_prediction.reshape(-1, 1)\n",
    "        )[0][0]\n",
    "        \n",
    "        # Apply constraints based on service group stats\n",
    "        if service_group in self.service_group_stats:\n",
    "            stats = self.service_group_stats[service_group]\n",
    "            max_val = stats['max']\n",
    "            constrained_bandwidth = max(min(bandwidth_prediction, max_val * 1.1), stats['min'] * 0.9)\n",
    "        else:\n",
    "            # Fallback to conservative estimation\n",
    "            constrained_bandwidth = bandwidth_prediction\n",
    "        \n",
    "        # Calculate allocation percentage based on maximum possible bandwidth\n",
    "        if service_group in self.service_group_stats:\n",
    "            max_bandwidth = self.service_group_stats[service_group]['max']\n",
    "            allocation_percentage = (constrained_bandwidth / max_bandwidth) * 100\n",
    "        else:\n",
    "            # Use a default assumption if stats not available\n",
    "            allocation_percentage = 75.0  # Conservative default\n",
    "        \n",
    "        # Ensure percentage is within reasonable bounds\n",
    "        allocation_percentage = max(min(allocation_percentage, 95), 10)\n",
    "        \n",
    "        # Get service group ID\n",
    "        group_id = self.service_mappings.get(service_group, {}).get(\"group_id\", 1000)\n",
    "        \n",
    "        # Get a service ID (any valid one for this group)\n",
    "        service_id = 100\n",
    "        for service_name, sid in self.service_mappings.get(service_group, {}).get(\"services\", {}).items():\n",
    "            service_id = sid\n",
    "            break  # Just take the first one\n",
    "        \n",
    "        # Format the results\n",
    "        result = {\n",
    "            \"service_group\": service_group,\n",
    "            \"group_id\": group_id,\n",
    "            \"service_id\": service_id,\n",
    "            \"bandwidth_allocation\": f\"{allocation_percentage:.2f}%\",\n",
    "            \"predicted_bandwidth_mbps\": f\"{constrained_bandwidth:.2f}\"\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def predict_all_groups(self, timestamp, usage_percentages=None, device_groups=None):\n",
    "        \"\"\"\n",
    "        Predict bandwidth for all service groups at a given time\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        timestamp : datetime\n",
    "            Timestamp for prediction\n",
    "        usage_percentages : dict, optional\n",
    "            Dictionary mapping service groups to usage percentages\n",
    "        device_groups : dict, optional\n",
    "            Dictionary mapping service groups to device groups\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Predictions for all service groups\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for service_group in self.models.keys():\n",
    "            usage = None\n",
    "            if usage_percentages and service_group in usage_percentages:\n",
    "                usage = usage_percentages[service_group]\n",
    "                \n",
    "            device = None\n",
    "            if device_groups and service_group in device_groups:\n",
    "                device = device_groups[service_group]\n",
    "            \n",
    "            try:\n",
    "                prediction = self.predict(timestamp, service_group, usage, device)\n",
    "                results[service_group] = prediction\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting for {service_group}: {str(e)}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_time_series_predictions(self, start_time, end_time, interval_hours=1):\n",
    "        \"\"\"\n",
    "        Generate predictions for a time range\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        start_time : datetime\n",
    "            Start timestamp for predictions\n",
    "        end_time : datetime\n",
    "            End timestamp for predictions\n",
    "        interval_hours : int or float, optional\n",
    "            Hours between prediction points\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Time series predictions in the format required for bandwidth allocation\n",
    "        \"\"\"\n",
    "        # Generate timestamps\n",
    "        current = start_time\n",
    "        predictions = {}\n",
    "        \n",
    "        while current <= end_time:\n",
    "            # Make predictions for this timestamp\n",
    "            formatted_timestamp = current.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # Determine the most relevant service group for this time\n",
    "            hour = current.hour\n",
    "            is_weekend = current.weekday() >= 5\n",
    "            \n",
    "            # Time-based service group probabilities\n",
    "            if 9 <= hour <= 17 and not is_weekend:\n",
    "                # Work hours on weekdays - Software dominates\n",
    "                service_probs = {\n",
    "                    \"Software\": 0.5,\n",
    "                    \"Streaming\": 0.1,\n",
    "                    \"Social Media\": 0.2,\n",
    "                    \"Shopping\": 0.1,\n",
    "                    \"Gaming\": 0.1\n",
    "                }\n",
    "            elif 18 <= hour <= 23:\n",
    "                # Evening hours - Streaming and Gaming dominate\n",
    "                service_probs = {\n",
    "                    \"Software\": 0.1,\n",
    "                    \"Streaming\": 0.4,\n",
    "                    \"Social Media\": 0.2,\n",
    "                    \"Shopping\": 0.1,\n",
    "                    \"Gaming\": 0.2\n",
    "                }\n",
    "            elif is_weekend:\n",
    "                # Weekend - Mixed with more entertainment\n",
    "                service_probs = {\n",
    "                    \"Software\": 0.1,\n",
    "                    \"Streaming\": 0.3,\n",
    "                    \"Social Media\": 0.2,\n",
    "                    \"Shopping\": 0.2,\n",
    "                    \"Gaming\": 0.2\n",
    "                }\n",
    "            else:\n",
    "                # Early morning - More Social Media\n",
    "                service_probs = {\n",
    "                    \"Software\": 0.2,\n",
    "                    \"Streaming\": 0.2,\n",
    "                    \"Social Media\": 0.4,\n",
    "                    \"Shopping\": 0.1,\n",
    "                    \"Gaming\": 0.1\n",
    "                }\n",
    "            \n",
    "            # Filter to available models\n",
    "            available_groups = set(self.models.keys())\n",
    "            service_probs = {k: v for k, v in service_probs.items() if k in available_groups}\n",
    "            \n",
    "            if not service_probs:\n",
    "                # No valid models found - just use the first available model\n",
    "                service_group = list(self.models.keys())[0]\n",
    "            else:\n",
    "                # Normalize probabilities\n",
    "                total = sum(service_probs.values())\n",
    "                service_probs = {k: v/total for k, v in service_probs.items()}\n",
    "                \n",
    "                # Weighted random choice of service group\n",
    "                groups = list(service_probs.keys())\n",
    "                weights = list(service_probs.values())\n",
    "                service_group = random.choices(groups, weights=weights, k=1)[0]\n",
    "            \n",
    "            # Predict for this service group\n",
    "            prediction = self.predict(current, service_group)\n",
    "            predictions[formatted_timestamp] = prediction\n",
    "            \n",
    "            # Move to next time interval\n",
    "            current += timedelta(hours=interval_hours)\n",
    "        \n",
    "        # Format as expected by the bandwidth allocation system\n",
    "        return {\"responseData\": predictions}\n",
    "\n",
    "    def save_predictions_to_json(self, predictions, output_file='edge_predictions.json'):\n",
    "        \"\"\"\n",
    "        Save predictions to a JSON file\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        predictions : dict\n",
    "            Prediction results\n",
    "        output_file : str, optional\n",
    "            Path to save the JSON file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(predictions, f, indent=2)\n",
    "            \n",
    "        print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the predictor\n",
    "    predictor = BandwidthPredictor()\n",
    "    \n",
    "    # Example 1: Predict for a specific timestamp and service group\n",
    "    current_time = datetime.now()\n",
    "    prediction = predictor.predict(current_time, \"Streaming\")\n",
    "    print(\"Prediction for current time:\")\n",
    "    print(json.dumps(prediction, indent=2))\n",
    "    \n",
    "    # Example 2: Generate predictions for the next 24 hours\n",
    "    start_time = datetime.now()\n",
    "    end_time = start_time + timedelta(hours=24)\n",
    "    \n",
    "    print(f\"\\nGenerating predictions from {start_time} to {end_time}\")\n",
    "    time_series = predictor.generate_time_series_predictions(start_time, end_time, interval_hours=2)\n",
    "    predictor.save_predictions_to_json(time_series, \"next_24_hours_predictions.json\")\n",
    "    \n",
    "    # Example 3: Predict for a custom date range\n",
    "    custom_start = datetime(2025, 3, 27, 8, 0, 0)  # March 27, 2025 at 8:00 AM\n",
    "    custom_end = datetime(2025, 3, 29, 23, 0, 0)   # March 29, 2025 at 11:00 PM\n",
    "    \n",
    "    print(f\"\\nGenerating predictions for custom date range: {custom_start} to {custom_end}\")\n",
    "    custom_predictions = predictor.generate_time_series_predictions(custom_start, custom_end, interval_hours=4)\n",
    "    predictor.save_predictions_to_json(custom_predictions, \"custom_date_range_predictions.json\")\n",
    "    \n",
    "    print(\"\\nAll predictions completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
